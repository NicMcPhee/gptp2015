% kramdown -o latex markdown/session.md > markdown/session.tex
% kramdown -o latex markdown/manuscript.md > markdown/manuscript.tex

\section{Why}\hypertarget{why}{}\label{why}

More than a decade ago, Rick Riolo, Bill Worzel and I were working on a consulting project together that involved genetic programming. As we chatted one day, Rick was asked what he'd most like to see as part of the research program of GP ``moving forward''. Rick's answer informs this contribution, as well as much of my professional work with GP in the years since.

As I recall, Rick said he'd like work to focus on the ``symptoms'' we often see in evolutionary search processes: premature convergence, failure to improve, catastrophic lack of diversity, mysterious things like that sense we all get when we look at results that suggests \emph{we made bad choices} of some sort.

The literature abounds with well-written papers describing tips for avoiding local minima, improving on common search operators, and describing ``horse races'' between Bad Old and Better New search methodologies applied to benchmark problems. But I take Rick's challenge not only to mean that it would be useful to have a catalog which lists situations where search operator $X$ acting under contingency $Y$ tends to produce outcome $Z$, but also that \emph{the things we identify as symptoms themselves} are poorly understood.

This contribution aims squarely at that sensibility which sees the ``symptoms'' and ``pathologies'' as something we as a research community should be trying to stamp out \emph{before} letting users run their own GP systems. Instead, I'll argue that because GP differs qualitatively and philosophically from most other machine learning approaches, and also from the broader lay understanding of the ``automated design'' and exploratory research, much of the value GP offers is lost or ignored when we approach it as something to be ``cleaned up''.

\subsection{Challenges}\hypertarget{challenges}{}\label{challenges}

Somewhere during that same project years ago, I remember a Project Manager recounting the story that when domain expert customers were shown a collection of Pareto-optimal solutions to the problem being explored, they were upset and confused: ``We don't want a choice, we want \emph{the best}.'' Of course there could be no \emph{best}, and they supposedly knew that; this response came after \emph{months} of analysis, interviews, technical planning and a collective agreement that the problem was fundamentally multi-objective. Even so, when the decision-makers were faced with the \emph{unquestionably} successful results they had collectively specified and specifically asked for\ldots{} those results weren't right enough.

I would much rather we faced the fact that this shocked response to \emph{seeing what's happening} is inevitable---not just in the form of unwitting surprise from silly lay ``customers'' in an application project, but also from ourselves when we undertake any \emph{interesting} GP project, whether the project is ``theoretical'' or ``practical'', ``small'' or ``large''.

Any \emph{interesting} project will resist our best expectations. Now and then we should question whether our habitual response to that inevitable resistance is in any sense effective.

The field of GP has grown quite a bit in the 15 years since my two anecdotes. We who follow it closely have watched as frameworks, techniques and methodologies have diversified. There are the extraordinary ones we describe to one another in our little workshops, but also an increasing number of newsworthy and admirable public successes.

But this acceleration of innovation brings with it a dilution of the theoretical warrants we use to justify results. Most practical successes today highlight unique domain-specific quirks. New language and framework implementations makes numerous contingent (and often arbitrary) design and architectural choices. Even the fundamental ontological concepts of ``individual'', ``fitness'', ``behavior'', ``generation'' and ``population'' start to get troubling when try to fit many modern algorithms into their categories.

This growing divergence between current theory and practice has consequences both outside and within our field. The continued lack of interest in GP among statisticians, planners, designers and fans of traditional mathematical programming models is not merely a matter of disciplinary border wars. As we increasingly lose the ability to say \emph{why} something is happening in a given GP project, we are faced with an audience who stopped paying attention to progress in GP last century. As a result, students and researchers aiming to convince peers outside the field are forced to undertake mismatched experiments and apply unconvincing statistical tests. In response to these constant distractions, much time and attention goes towards unhelpful work guaranteeing that which cannot be guaranteed: running ``replicates'' of a process that is \emph{designed} to provide novel answers, measuring ``reliability'' of a process that \emph{intentionally} skirts dynamical chaos, providing ``summary charts'' of a process that strives to be as complex as the evolution of living beings, and (possibly worst) setting strict deadlines for an open-ended process to ``finish'' or ``succeed''.

I hasten to say this is not a fault in our field, but rather a broader philosophical and cultural problem. But it is nonetheless a misconception that undermines our understanding of what GP is \emph{doing}, of the way it unfolds in theory and in practice, and even what it's \emph{for}.

\subsection{A top-down approach}\hypertarget{a-top-down-approach}{}\label{a-top-down-approach}

I will make my case here in the form of an exercise for advanced GP users, or \emph{kata}. The habit of pursuing \emph{kata}, ``code retreats'', ``hackathons'' and other skill-honing practices is popular among software developers, and especially among the more advanced. Indeed, the title of my exercise (``GP as if you meant it'') is taken from a particularly influential exercise designed by Keith Braithwaite, ``TDD as if you meant it''.

Braithwaite's exercise feels subjectively \emph{harder} for more advanced programmers honing their development form; he suggests that novice programmers haven't learned ingrained but questionable habits, and haven't identified ``shortcuts'' that ``simplify'' the practices. In the same way, this exercise will feel \emph{most} artificial and restrictive to those of us with the most experience with GP. But like the martial arts exercises from software \emph{kata} were inspired, it isn't intended to be simple or even pleasant for the participants.

I intend it to expose habits I'll tactlessly call ``psueudo-GP'' among those of us who have learned through the years to think it's cheap and painless to \emph{just shut it off and start over} when things start to get strange in the course of a GP project. As an important side-effect, it helps surface that philosophical problem I alluded to above. But I have found the most useful result is how strongly and immediately it suggests tools with which one can address Rick's decade-old wish. It forces the participant to formally identify ``pathology'' and ``symptom'' before allowing them to attempt a ``fix''\ldots{} and even that must done with an intentionally limited set of tools.

Note, what I'll describe is a ``thought-experiment'', nor is it a serious suggestion for a new way of working on ``real problems'', nor as ``training'' for newcomers to the field. Rather it is designed as a rigorous and formal exercise to be undertaken by those of us already working closely with GP systems. Its intent is to surface the three shortcomings I've identified above:

\begin{enumerate}
\item what GP \emph{does}
\item how GP \emph{unfolds}
\item what GP is \emph{for}
\end{enumerate}

\section{How we treat GP, and how it treats us in turn}\hypertarget{how-we-treat-gp-and-how-it-treats-us-in-turn}{}\label{how-we-treat-gp-and-how-it-treats-us-in-turn}

Genetic Programming\footnote{And not just Genetic Programming as such, but also the broader discipline to which I claim it belongs and which is not obliged to be either ``genetic'' or ``programming''. I prefer to call this looser collection of practices ``generative processing'', and will also abbreviate it ``GP''; assume I mean the latter in every case.} embodies a very particular \emph{stance} towards the scientific and engineering work of modeling, design, analysis and optimization. I increasingly suspect the resistance we've all recounted towards GP from our prospective technical and lay audience has little to do with our technical results, but rather arises from uncertainty among that audience with GP's very particular ``way of working''.

There is a tacit assumption, even among GP theorists and practitioners, that science and engineering are only ``rigorous'' when they proceed through a sequence of ordered phases from planning to implementation. The ``scientific method'' is most often represented something along the lines of

\begin{quote}
\begin{enumerate}
\item conceptualization; 2. planning; 3. design; 4. architecture; 5. implementation; 6. testing; 7. debugging
\end{enumerate}
\end{quote}

I am sure very few scientists or engineers of my acquaintance would admit any \emph{real} project (in history) has ever followed this narrative arc in a literal sense. But that story nonetheless informs and constrains much of our work lives, from fund-raising to publishing reports:

\begin{quote}
Because of the body of published work, an insight was had. The insight was framed as a formal hypothesis. The hypothesis (and current Best Statistical Practices) suggested an experimental design, one familiar and obvious to any in Our Discipline. The experimental design was undertaken, the data were collected, the hypothesis duly tested, and now we can be confident of its veracity because\ldots{} well, you just heard me say ``Best Statistical Practices'', right?
\end{quote}

Under trivial term substitutions---``cost--benefit analysis'' and ``requirements document'' for ``hypotheses'' and ``experimental design'', for example---the same narrative can be used to describe almost any institutional project management and public policy planning process as well. The flow in every case is essentially from \emph{vision} to \emph{plan}, \emph{plan} to \emph{implementation}, \emph{implementation} to \emph{verification}, and \emph{verification} to \emph{validation}.

Of course, nobody ``really believes'' this narrative who has ever done any of the work. It is a matter for another day to draw parallels with the social construction of religious belief.\footnote{Paul Veyne's excellent \emph{Did the Greeks Believe in Their Myths?} might be an interesting starting point, though.} There have been numerous philosophical challenges to this artificial narrative of course, from Peirce and Dewey nearly a century ago, to Kuhn and Lakatos in the 1970s, and many more to be found in the Table of Contents in any Philosophy of Science text.

One in particular is my focus today: Andrew Pickering.

\subsection{Pickering's ``Mangle of Practice''}\hypertarget{pickerings-mangle-of-practice}{}\label{pickerings-mangle-of-practice}

Andrew Pickering's \emph{Mangle of Practice} is a decade old, but surprisingly little-known outside the field of Science Studies. His approach is especially noteworthy here because I find it so close to our actual experience using GP. Indeed, most colleagues who hear it for the first time utter an inevitable ``didn't we already know this?'' I think the distinction Pickering's approach is able to highlight is exactly the problematic one in our work: between the illusory (but publishable) narrative of ``scientific method'', and the realized experience we have of \emph{performing science}. At the cost of glossing a lot of his well-considered structure, let me summarize.

First, the act of ``doing science'' is \emph{at no point whatsoever} to be understood as one in which an isolated ``researcher'' works in an objective and static field of ``externalities'' and ``facts''. Rather, research begins only when the researcher undertakes to \emph{makes some artifact}: a block of code, a technical instrument, an equation, a sketch, a maquette or even a thoughtful conversation at a conference. Everything before the researcher begins to make these things is a matter of building ``vision'' (my term); research proper only occurs when work is done in the real world. I'll call this work product \emph{the thing made}, and keep in mind that it is a proxy for the vague collection we might otherwise call ``the project''.

We see Pickering's model moving quickly away from more traditional views of science when we realize he has given the inanimate \emph{thing made} an agency of its own. In any real project, we perceive the \emph{thing made} resisting---whether we imagine that it resists ``in itself'', or as an agent of externalities that impinge on the work in progress to make our driving vision differ from reality. The \emph{thing made} comes to represent, in other words, the facts of the actual world, the cultural assumptions and norms of one's discipline, the raw materials and toolkit available to a practitioner, and so forth. ``Resistance'' here is not merely a problem with a software bug or lost raw materials, but includes one's sense \emph{on seeing it} that something's not quite right. The \emph{thing made}'s resistance, as perceived by the researcher, leads her to react in turn.

Consider the language we use in the face of this resistance: it ``feels wrong''; it ``points something out''; it ``wants to do X instead of Y''; it's ``doing something too complicated for me to understand right now''.

And---assuming science, engineering, art or any other creative process is indeed what's being done---the researcher \emph{changes in response to this resistance}. That vision changes, the plan adapts, or in some other way the \emph{thing made} causes a response in the state of the researcher herself. Pickering's Mangle is this emergent dance of inanimate agency: the researcher starting to follow a vision by making (or altering) a \emph{thing}, and the \emph{thing made} in turn acting as a channel for the world itself to steer the researcher in another direction.

Pickering's ``mangle''\footnote{The word ``mangle'' he has chosen is itself interesting and insightful:

\begin{quote}
\ldots{}I find ``mangle'' a convenient and suggestive shorthand for the dialectic because, for me, it conjures up the image of the unpredictable transformations worked upon whatever gets fed into the old-fashioned device of the same name used to squeeze the water out of the washing. It draws attention to the emergently intertwined delineation and reconfiguration of machinic captures and human intentions, practices, and so on. The word ``mangle'' can also be used appropriately in other ways, for instance as a verb. Thus I say that the contours of material and social agency are mangled in practice, meaning emergently transformed and delineated in the dialectic of resistance and accommodation\ldots{}.
\end{quote}} is this emergent dance of agency, undertaken between the researcher and the project: the researcher makes, the \emph{thing made} resists, the researcher is influenced and redirected, \emph{accommodating} the resistance. It may seem glib to say that ``no plan survives contact with the enemy,'' but Pickering's narrative of creative work emphasizes the fact that no \emph{revised} plan survives unchanged, either. In the traditional narrative, we elide the work as it unfolded and re-frame it as a sort of idealized, apersonal Platonic truth: we use the passive voice, we hide the missteps and confusion, we paint a story moving from vision to plan to success.

In a GP setting, the many modes of resistance we perceive are the very ``symptoms'' Rick mused about. Even though we as researchers know we've written all the code and set all the parameters, we're nonetheless willing to speak of a GP run ``doing'' things, as opposed to merely unfolding according to our plan. A GP system does not ``resist'' by, for example, ``having the wrong population size''. Rather it resists by \emph{causing concern or dissatisfaction in the user}, which in turn sparks in that user a practical (if only explanatory) response, which leads them to change the \emph{thing made}.

\subsection{GP as ``mangle-ish practice''}\hypertarget{gp-as-mangle-ish-practice}{}\label{gp-as-mangle-ish-practice}

The broader field of machine learning seems to take a much more traditional stance towards its subject matter: machine learning frameworks (excepting GP) are each discrete tools aimed at producing standardized and reproducible results to particular statistical questions. The result of training a neural network or even a random forest on a given data set is not expected to be a \emph{surprise} in any real sense, but rather the reliable and robust end-product of applying numerical optimization to a well-specified mathematical programming problem. Whether one describes these machine learning processes as ``minimizing out-of-sample error'' or ``maximizing information gain'', the supposed strength of most machine learning approaches is the \emph{unsurprising} nature of their use cases and outputs.

On the other hand, GP has the capacity to \emph{tell us stories}---even in the relatively ``simple'' domain of symbolic regression. The space under consideration by GP is not merely a vector of numerical constants or a binary mask over a suite of input variables, but the \emph{power-set} of inputs, functions over inputs, and higher-order functions over those. We who work in the field can be glib about the ``open-endedness'' of GP systems, but that open-endedness puts them at odds with their supposed relatives in machine learning. While GP \emph{can} be used to explore arbitrarily close to some paradigmatic model, its more typical use case leads to the production of \emph{unexpected} insights---to the degree that a number of us feel justified in treating it as a strong candidate for ``real'' artificial intelligence.

I argue we have that leeway because of the way GP surfaces Pickering's Mangle. When the methodology ``works'', it does so by offering \emph{helpful resistance} in our engagement with the problem at hand, whether in the form of surprising answers, or validation of our suspicions, or simply legible suggestions of ways to make subsequent moves. GP \emph{dances} with us, while most other machine learning methods are ``mere tools''.

\subsection{Against replication}\hypertarget{against-replication}{}\label{against-replication}

Nonetheless, there seems to be a widespread desire, inside and outside our field, to frame GP as a methodology for producing \emph{unsurprising} models from data, more in keeping with the traditional linear of scientific work. That is, an idealized user is expected to proceed something like the users of any other machine learning or mathematical programming framework:

\begin{enumerate}
\item frame your problem in the correct formal language
\item ``get'' a GP system
\item run GP ``on your data''
\item (whatever this is, it's not our problem)
\item you have solved your problem
\end{enumerate}

This is not original with the GP community; there is strong pressure from our peers in other disciplines and our users to promote this use case, not least because it is \emph{exactly} the stance expected in any planning or public policy setting, or in any scientific or programming project management setting. That is, we are under tremendous social pressure to treat GP as a \emph{tool} to be invoked in a known, predictable and well-described planning situation.

The resulting resistance from early-adopting users shouldn't be unexpected. \emph{Being surprising} may well be the worst conceivable behavior for any tool to be used in a traditional project setting. And given that pressure, it's no wonder that so much of GP research is focused on constraining tweaks to bring GP ``into line''. If only GP could be ``tamed'' or made ``adaptive'' so that step (4) above \emph{never happens}.

I imagine this is why so many GP research projects strive for rigor in the form of counting replicates which ``find the solution'': they aim not to convince users of the known strengths of GP, but rather demonstrate to critical peers that GP can be ``tamed'' into a mere tool.

What would a ``replicate'' stand for, to a user who sought to exploit GP's strength in a project (whether theoretical or practical) where \emph{search} rather than the algorithm itself is the primary focus? Projects which authentically ``use'' GP \emph{must necessarily} be those searching for noteworthy answers---which is to say \emph{surprising} and \emph{interesting} answers---that they could not otherwise obtain. Thus, we should better think of a ``replicate'' as a sort of proxy for user frustration in step (4) above: that is, it represents a project in which search begins, stalls, and for which the user cannot see a way to move search forward towards more interesting and useful answers.

But I think we would agree: it is a poor researcher who, when faced with a stumbling block in the form of a black box's misbehavior, doesn't attempt to work around that obstacle. Not to \emph{begin the project again from scratch}, but to make changes and continue. Any researcher who is using GP \emph{realistically} will be watching, and adjusting, and engaging and interacting with the process of search itself.

When a GP user is viewed as working within the traditional \emph{linear} narrative of research, we see her run a population of 100 individuals for 100 generations, peer at the results that have been dumped into a CSV file, and find them wanting. Desiring an actual \emph{answer}, she adjusts the GP parameters and begins another run of 100 generations\ldots{} and repeats as necessary.

But there is \emph{no discernible difference} when we frame this same process of ``many runs'' as a single project involving initial researcher moves, resistances thrown up by the \emph{thing made}, and subsequent accommodations made by the researcher to the new perceived truths. Except, that is, in the researcher's view of her own response to resistance: if she comes to the project with \emph{plans} for running ``ten replicates'', we can only assume she has learned from someone that a search algorithm is \emph{supposed} to forget everything it has learned every 100 generations\ldots{}.

I cannot help but be reminded of the fallacy, surprisingly common both in and outside of our field, that ``artificial intelligence'' must somehow be a self-contained and non-interactive process. That is, that an ``AI candidate'' loses all authenticity as soon as it is ``tweaked'' or ``adjusted'' in the course of operation. It is as if every new-born ``AI'' must be jammed into an air-tight computational container and isolated \emph{until it learns to reason}, and that without exceeding a finite computational budget. If humans creating \emph{real} intelligences treated them anything like the way computer scientists insist we treat nascent \emph{artificial} ones, murder charges would be forthcoming.

There are several practical reasons for us to try something different.

Consider our hypothetical GP user. In keeping with the Behaviorist standards of GP, she is carefully ``not interfering'' with any given run of 100 generations; she can only peer at a results file after the fact. During the course of any 100 generations, all sorts of dynamics have happened: crossover, mutation, selection, all the many random choices. Imagine for a moment we were given perfect access to the entire dynamical pedigree of the unsatisfying results she receives at the end, and were able to backtrack to any point in the run and change a single decision. Before that point, it's unclear how badly things will actually turn out at the 100-generation mark; at some point after that juncture, it's obvious to anybody watching that the whole thing's a mess.

If such miraculous insights were available, then surely the correct approach would be to intervene and adjust the situation when the crucial point was reached\ldots{} and then continue. Lacking (as we do) this miraculous insight, \emph{why then does it seem reasonable to stop any run arbitrarily at a pre-ordained time point and begin again from scratch?}

It is, I think, because the myths of artificial intelligence and the linear narrative of science are so deeply intertwined. It is frowned upon to admit in a scientific paper, even when no mistakes were made, that the original vision and plan changed over the course of the project; rather we are expected to describe research \emph{results} as the inevitable outcomes of an ahistorical process, and erase all resistance and accommodation actually done by human beings in the context of their projects. Similarly, it feels somehow wrong to admit in a GP project, even if every parameter was set correctly, that the original vision and plan gave way to the inevitable surprises thrown up by GP's inherent tendencies to do just that.

But insofar as GP \emph{surprises us}, and since that is its sole strength over more predictable and manageable frameworks, we must inevitably see a good fraction of those surprises at least in part as \emph{disappointments} rather than encouraging opportunities to change our plans.  Let's learn new ways to accommodate those disappointments, and stop trying to make them go away.

\section{``TDD as if you meant it''}\hypertarget{tdd-as-if-you-meant-it}{}\label{tdd-as-if-you-meant-it}

As far as I can tell, Keith Braithwaite first described his \href{http://cumulative-hypotheses.org/2011/08/30/tdd-as-if-you-meant-it/}{training exercise} for software developers in 2009. The target of the exercise is ``Pseudo-TDD'': the noted habit among software developers who claim to ``know'' and ``do'' test-driven development as part of their daily work towards a sort of thoughtless approximation of the technique.

I should note that a number of agile software development practices share informative relations to genetic programming's dynamics\footnote{I imagine there is an Engineering Studies thesis in this for some aspiring graduate student: Genetic programming and agile development practices arose in the same period and more or less the same culture, and both informed by the same currents in complex systems and emergent approaches to problem-solving.}, but in this work I'll focus on those of TDD. In particular, test-driven development (or more accurately ``test-driven design'') \emph{when done correctly} can break down the complex design space of a software project into a value-ordered set of incremental test cases, focus the developers' attention on those cases alone, inhibit unnecessary ``code bloat'' and feature creep, and produce low-complexity understandable and maintainable software.

TDD \emph{as such} is a rigorous process, to the point where it can be described as ``painful'' (though also ``useful'') by experienced programmers. The steps are deceptively easy to trivialize and misunderstand, especially for those whose habits of thinking about code are ingrained:

\begin{enumerate}
\item Add a little (failing) test which exercises the next behavior you want to build into your codebase
\item Run all tests, expecting only the newest to fail
\item Make the minimal change to your codebase that permits the new test to pass
\item Run all tests, expecting them all to succeed
\item Refactor codebase to remove duplication
\end{enumerate}

Each stage offers a stumbling block for an experienced programmer, but the most salient for us now is the iterative flow of implementation (or ``design'') that it imposes: Each cycle begins with a choice of \emph{which little test should next be added}; each cycle ends with a rigorous process of refactoring, not just of the new code but of the \emph{entire cumulative codebase} produced so far. The middle three steps---implementing a \emph{single} failing test and modifying the codebase \emph{by just enough} so that all tests pass---feel when one is working through them as if they could be automated easily. The \emph{mindfulness} of the process lives in the choice of next steps and (though somewhat less so) of standard refactoring operations.

Braithwaite's exercise does an interesting thing to surface the formal rigor of this approach. In it, the participants (willing, of course, because the exercise is a \emph{kata} or ``refresher'' for experienced software developers to hone their skills) are asked to implement a nominally simple project like the game of Tic-Tac-Toe, given an \emph{ordered} list of features to implement and the artificial restriction that they must go farther than normal TDD practice asks. Rather than producing a suite of tests and a self-contained codebase, they are forced to use \emph{only} refactoring of code added to tests to produce their eventual ``codebase''. In other words, no code can be ``produced'' until the ``smell'' of duplication in the code added to multiple passing tests \emph{provides a warrant} for refactoring it out.

Further, a facilitator patrols ongoing work and deletes \emph{any and all code not called for by a pre-existing failing test}. Words like ``irritating'' and ``annoying'' crop up in participants' accounts of  this onerous backtracking deletion the first few times it happens, as one might imagine. But as Gojko Adzik emphasizes in his descriptions of workshops, the resulting designs for even simple algorithms in this artificially amplified setting seems much more \emph{open-ended} than it would if the software were built under the typical norms and habits an experienced programmer uses in normal conditions.

A number of contextually positive benefits are attributed to agile software development practices, and to TDD within that suite of practices. But the one that brings us here today is that aspect surfaced particularly in Adzik's account of Tic-tac-toe:

\begin{quotation}
By the end of the exercise, almost half the teams were coding towards something that was not a $3\times 3$ char/int grid. We did not have the time to finish the whole thing, but some interesting solutions in making were:

\begin{itemize}
\item a bag of fields that are literally taken by players---field objects start in the collection belonging to the game and move to collections belonging to players, which simply avoids edge cases such as taking an already taken field and makes checking for game end criteria very easy.
\item fields that have logic whether they are taken or not and by whom
\item game with a current state field that was recalculated as the actions were performed on it and methods that could set this externally to make it easy to test
\end{itemize}
\end{quotation}

In other words: innovative approaches to the problem at hand began to arise, though there wasn't enough time to finish them in the time alloted for the exercise. The analogy to our collective experience to date with genetic programming should start to peek through at this point---though the thoughtful reader will hopefully wonder what utility there is in an analogy drawn between two similarly unsatisfying outcomes.

[more here]

\section{GP as if we meant it}\hypertarget{gp-as-if-we-meant-it}{}\label{gp-as-if-we-meant-it}

In the same way that Braithwaite's onerous coding exercise is intended to drive the attention of its participants toward test-driven design with its obligation to write ``real'' code \emph{only as a refactoring}, I'd like to be able to demand a \emph{warrant} for every step that moves our changing genetic programming setup away from just plain random guessing. Braithwaite's target of ``Pseudo-TDD'' suggests an analogous ``Pseudo-GP'': one in which the fitness function is the only ``interface'' with the problem itself, and where the representation language, search operators, search objectives and other algorithmic ``parameters'' are \emph{fixed}.\footnote{Braithwaite's participants often acknowledge they \emph{know} and \emph{use} TDD as it's formally described, but rarely take the time to do so unless ``something goes wrong''. I imagine many GP users will say they \emph{know} and \emph{use} all the innumerable design and setup options of GP, but treat them as adjustments to be invoked only when ``something goes wrong''. I offer no particular justification for either anecdote here, but the curious reader is encouraged to poll a sample of participants at any conference (agile or GP).}

Not only do traditional search operators like crossover, mutation and [negative] selection not come ``for free'' in this variant, but in every case we must develop a cogent, data-driven argument in favor of starting them \emph{as part of an ongoing search process}. Similarly, the initial selection criteria will be limited to a minimal subset of the training data, and expansion (and other alterations) of the training set will have to be made in light of measured progress, not assumptions that ``more is better'' in every case.

The result will be an incremental process of refinement of an ongoing search, carried out not at the level of externally-assigned parameter ``tweaks'' but rather by \emph{opening} the black boxes we typically demand and demanding we do surgery to correct their ``pathologies'' (and understand their mysteries) without killing them outright. It is not intended as an ``algorithm'' to supplant those used today, but rather as a forced re-description of what we actually already do.

\subsection{A tableau representation}\hypertarget{a-tableau-representation}{}\label{a-tableau-representation}

The exercise proceeds as a sort of game, in which the User and the System take alternating turns. During the User's turn, she can see state of the system so far and make any of several \emph{moves} from a limited set, each of which involve changing the settings of a \emph{tableau}, which completely describes the state of the search system. During the System's turn, it will execute a finite number of steps in which it creates new individuals. At the end of the System's turn, control reverts to the User, and vice versa.

The tableau, and the suite of moves available to the User, affect three core aspects of search: {\tt operators}, {\tt answers}, and {\tt rubrics}.

\subsubsection{Operators}\hypertarget{operators}{}\label{operators}

An {\tt operator} is any function which takes as argument a (possibly empty) collection of {\tt answers} and produces a new collection of {\tt answers}. Operators thus include ``random guessing'', which in GP systems is often used to build an initial population, any ``crossover'' and ``mutation'' functions. No {\tt operator} can refer to any {\tt answer} not part of its argument set, not can any unset values be set within an {\tt operator}.

So for example, if the User wished an {\tt operator} could be constructed which took 30 {\tt answers} as inputs, read their attributes (including {\tt script} and that subset of their {\tt rubric} values which were already set), and produce a single {\tt answer} in its return set. It could \emph{not} take an {\tt answer}, change it, and keep the new one ``if it was better'', since the new {\tt answer} would have no scores at all.

Note that no process is provided for {\tt answers} to be \emph{removed} from a tableau. The framework is purely cumulative.

All ``parents'' and other {\tt answer} inputs required by an {\tt operator} are chosen by \emph{lexicase selection} automatically, using the complete suite of {\tt rubrics} in play when invoked. Lexicase selection samples each {\tt rubric} with equal probability.

\subsubsection{Answers}\hypertarget{answers}{}\label{answers}

An {\tt answer} might traditionally be called an ``individual'' in GP literature. We can model them programmatically as key--value hash. All new {\tt answers} are born with only a unique {\tt id} and {\tt script} field set. As an {\tt answer} ``matures'' in the unfolding tableau, various other attributes will be set by the System player through the application of {\tt rubrics}.

In the tableau visualization, we represent the unfolding collection of {\tt answers} as the \emph{rows} of a spreadsheet-like table, and their attributes and scores as the \emph{columns}.

\subsubsection{Rubrics}\hypertarget{rubrics}{}\label{rubrics}

A {\tt rubric} is any function which returns a scalar numerical value, given the instantaneous state of the tableau itself. For the most part these values can be understood as ``scores'' for various search objectives, though it is likely that a number of ``implicit objectives'' will also create new {\tt rubrics}.

A {\tt rubric} cannot store intermediate values, but can refer to the values of other {\tt rubric} columns. Thus on specifying a {\tt rubric} like ``sum squared error over a training set'', one must also create  {\tt rubrics} for ``measured error when provided input $i$'' for every element $i$ of the training set. If the training set has 100 elements, then the single SSE {\tt rubric} \emph{implicitly} represents a suite of 101 new columns added to the tableau.

A {\tt rubric} function does however have access to the state of the entire tableau as needed, including ``forcing'' other {\tt rubric} values to be calculated. So it is entirely possible to specify {\tt rubric} functions which score:

\begin{itemize}
\item number of characters in the {\tt answer}'s script
\item maximum error measured in any of 35 other {\tt rubrics}
\item number of {\tt div0} errors produced when running with a particular set of inputs
\item number of stochastic instructions appearing in the {\tt answer}'s script, compared to \emph{all other {\tt answers}}
\end{itemize}

\subsubsection{User moves}\hypertarget{user-moves}{}\label{user-moves}

If we think of the tableau as a ``spreadsheet'' with a small list of {\tt operators} and a large sheet of {\tt answers} as rows and {\tt rubrics} as columns, the User can add one new {\tt operator} or one new column.

\begin{itemize}
\item add (or activate from a pre-existing list) exactly one {\tt operator}
\item add exactly one {\tt rubric}, which may in turn create other \emph{entailed} {\tt rubrics} as described above
\end{itemize}

\subsubsection{System turn}\hypertarget{system-turn}{}\label{system-turn}

During its turn, the System player adds a specified minimum number of new {\tt answers} to the tableau. If we think of the tableau as a ``spreadsheet'' the System can only add new {\tt answers}. It follows a single rote cycle to do so:

\begin{enumerate}
\item select an {\tt operator} from those in play, with equal probability
\item apply \emph{lexicase selection} to the tableau to select the required number of input {\tt answers}, filling in missing {\tt rubric} scores as needed
\item apply that one {\tt operator} to the inputs to produce new {\tt answers}, and append those new {\tt answers} immediately to the tableau
\item {\tt HALT} if the number of new {\tt answers} added in this turn meets or exceeds the number present when the turn started (in other words, if the number of {\tt answers} has doubled); otherwise, go to step (1)
\end{enumerate}

\subsubsection{Initial setup and restrictions}\hypertarget{initial-setup-and-restrictions}{}\label{initial-setup-and-restrictions}

\begin{itemize}
\item the only {\tt operator} is ``random guess'', which creates a new {\tt answer} with an arbitrary script
\item no {\tt rubrics} exist (only the {\tt script} and {\tt id} attributes of the {\tt answers})
\item 50 new {\tt answers} will be produced in the first System turn
\item No {\tt operator} can \emph{evaluate} any {\tt answer} except in a self-contained local namespace which lacks access to any tableau values or state information
\item There is no mechanism for \emph{removing} {\tt answers} from the tableau
\item The System player \emph{always} uses lexicase selection, and \emph{always} uses all {\tt rubrics} as the selection features with equal probability.
\item A {\tt rubric} can only \emph{run} a single {\tt answer} once; stochastic scripts will only be sampled one time, and no {\tt rubric} score is ever recalculated after the first time
\end{itemize}

\subsubsection{Interface: the Goal of the Exercise}\hypertarget{interface-the-goal-of-the-exercise}{}\label{interface-the-goal-of-the-exercise}

During the User's turn, they can of course interrogate the tableau in any way they want, without changing it. The point of the exercise is to drive the User to explore and create analytics and visualizations which can better inform their decisions over the course of the game.

\section{An example session}\hypertarget{an-example-session}{}\label{an-example-session}

\section{Exploration and exploitation interfaces and affordances}\hypertarget{exploration-and-exploitation-interfaces-and-affordances}{}\label{exploration-and-exploitation-interfaces-and-affordances}

\section{Final thoughts: What should it mean to \emph{act intelligently}?}\hypertarget{final-thoughts-what-should-it-mean-to-act-intelligently}{}\label{final-thoughts-what-should-it-mean-to-act-intelligently}

The idea of this exercise came from observing the differences between experienced and ``novice'' (though with strong technical skills) GP users as they explored new problems in a GP setting.

